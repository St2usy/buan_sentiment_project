# -*- coding: utf-8 -*-

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import pandas as pd
import time
import random
import os
from multiprocessing import Pool
import numpy as np
import json
from datetime import datetime

def init_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("user-agent=Mozilla/5.0")
    return webdriver.Chrome(options=options)

def process_urls(url_chunk):
    driver = init_driver()
    results = []
    
    for _, row in url_chunk.iterrows():
        try:
            print(f"  ì²˜ë¦¬ ì¤‘: {row['title'][:30]}...")
            content = crawl_blog_content(driver, row['url'])
            
            # ê°ì„± ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° êµ¬ì¡°
            blog_data = {
                "category": row['category'],
                "keyword": row['keyword'],
                "title": row['title'],
                "url": row['url'],
                "content": content,
                "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "sentiment_analysis": {
                    "positive_score": None,
                    "negative_score": None,
                    "neutral_score": None,
                    "overall_sentiment": None,
                    "keywords": [],
                    "analyzed_at": None
                }
            }
            results.append(blog_data)
            time.sleep(random.uniform(1, 2))
        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            results.append({
                "category": row['category'],
                "keyword": row['keyword'],
                "title": row['title'],
                "url": row['url'],
                "content": "",
                "error": str(e)
            })
    
    driver.quit()
    return results

def crawl_blog_content(driver, url):
    try:
        driver.get(url)
        time.sleep(random.uniform(2, 4))
        driver.switch_to.frame("mainFrame")
        content = driver.find_element(By.CSS_SELECTOR, "div.se-main-container").text
        return content
    except Exception as e:
        print("âŒ ì—ëŸ¬ ë°œìƒ:", e)
        return ""

def process_category(cat):
    print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬: {cat}")
    input_path = os.path.join("result", cat, "blog_urls.csv")
    output_path = os.path.join("result", cat, "blog_data.json")  # JSON í˜•ì‹ìœ¼ë¡œ ë³€ê²½

    if not os.path.exists(input_path):
        print(f"âŒ {input_path} ì—†ìŒ")
        return

    df = pd.read_csv(input_path)
    # ì²˜ìŒ 4ê°œì˜ URLë§Œ ì²˜ë¦¬
    df = df.head(4)
    
    # URLì„ 2ê°œì˜ ì²­í¬ë¡œ ë‚˜ëˆ” (ê°ê° 2ê°œì”©)
    url_chunks = np.array_split(df, 2)
    
    print(f"ğŸ” í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {cat} ì¹´í…Œê³ ë¦¬ì—ì„œ 4ê°œì˜ URLë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤")
    # 2ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¡œ ë³‘ë ¬ ì²˜ë¦¬
    with Pool(2) as pool:
        all_results = pool.map(process_urls, url_chunks)
    
    # ê²°ê³¼ ë³‘í•©
    results = [item for sublist in all_results for item in sublist]
    
    # JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}")

if __name__ == "__main__":
    print("ğŸ” í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ê° ì¹´í…Œê³ ë¦¬ì—ì„œ 4ê°œì˜ URLë§Œ í¬ë¡¤ë§í•©ë‹ˆë‹¤")
    base_path = "result"
    categories = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]
    
    # ê° ì¹´í…Œê³ ë¦¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸ìš©ì´ë¯€ë¡œ ë³‘ë ¬ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ)
    for cat in categories:
        process_category(cat)
    
    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!") 