# -*- coding: utf-8 -*-

import json
import os
from datetime import datetime
from konlpy.tag import Okt
from collections import Counter
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification

class SentimentAnalyzer:
    def __init__(self):
        # KcELECTRA ê¸°ë°˜ ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ
        self.model_name = "beomi/KcELECTRA-base"
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)
        
        self.sentiment_pipeline = pipeline(
            "sentiment-analysis",
            model=self.model,
            tokenizer=self.tokenizer
        )
        self.okt = Okt()
        
    def split_text(self, text, max_length=500):
        if not text:
            return []
            
        # í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ì—¬ ë¶„í• 
        try:
            tokens = self.tokenizer.tokenize(text)
            chunks = []
            current_chunk = []
            current_length = 0
            
            for token in tokens:
                current_chunk.append(token)
                current_length += 1
                
                if current_length >= max_length:
                    chunks.append(self.tokenizer.convert_tokens_to_string(current_chunk))
                    current_chunk = []
                    current_length = 0
            
            if current_chunk:
                chunks.append(self.tokenizer.convert_tokens_to_string(current_chunk))
                
            return chunks
        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘ ì—ëŸ¬: {e}")
            return [text]  # ë¶„í•  ì‹¤íŒ¨ ì‹œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì²˜ë¦¬
        
    def preprocess_text(self, text):
        if not text:
            return ""
            
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            words = self.okt.morphs(text, stem=True)
            return ' '.join(words)
        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
            return text
    
    def analyze_sentiment(self, text):
        if not text:
            return {"label": "neutral", "score": 0.5}
            
        # ê¸´ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ì—¬ ì²˜ë¦¬
        chunks = self.split_text(text)
        sentiments = []
        
        for chunk in chunks:
            try:
                result = self.sentiment_pipeline(chunk)[0]
                sentiments.append({
                    "label": result['label'],
                    "score": result['score']
                })
            except Exception as e:
                print(f"âŒ ì²­í¬ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
                continue
        
        if not sentiments:
            return {"label": "neutral", "score": 0.5}
            
        # ì „ì²´ ê°ì„± ì ìˆ˜ í‰ê·  ê³„ì‚°
        positive_scores = [s['score'] for s in sentiments if s['label'] == 'positive']
        negative_scores = [s['score'] for s in sentiments if s['label'] == 'negative']
        
        if positive_scores and negative_scores:
            avg_positive = sum(positive_scores) / len(positive_scores)
            avg_negative = sum(negative_scores) / len(negative_scores)
            if avg_positive > avg_negative:
                return {"label": "positive", "score": avg_positive}
            else:
                return {"label": "negative", "score": avg_negative}
        elif positive_scores:
            return {"label": "positive", "score": sum(positive_scores) / len(positive_scores)}
        elif negative_scores:
            return {"label": "negative", "score": sum(negative_scores) / len(negative_scores)}
        else:
            return {"label": "neutral", "score": 0.5}
    
    def extract_keywords(self, text, n=10):
        if not text:
            return []
            
        try:
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            words = self.okt.nouns(text)
            word_counts = Counter(words)
            return [word for word, _ in word_counts.most_common(n)]
        except Exception as e:
            print(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì—ëŸ¬: {e}")
            return []
    
    def process_category(self, category_path):
        print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì¤‘: {os.path.basename(category_path)}")
        
        try:
            # JSON íŒŒì¼ ì½ê¸°
            with open(os.path.join(category_path, "blog_data.json"), 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì¤‘ ì—ëŸ¬: {e}")
            return
        
        results = []
        for item in data:
            if not item.get('content'):
                continue
                
            try:
                # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
                processed_text = self.preprocess_text(item['content'])
                
                # ê°ì„± ë¶„ì„
                sentiment = self.analyze_sentiment(processed_text)
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = self.extract_keywords(processed_text)
                
                # ê²°ê³¼ ì—…ë°ì´íŠ¸
                item['sentiment_analysis'] = {
                    "positive_score": sentiment['score'] if sentiment['label'] == 'positive' else 1 - sentiment['score'],
                    "negative_score": sentiment['score'] if sentiment['label'] == 'negative' else 1 - sentiment['score'],
                    "neutral_score": 1 - abs(sentiment['score'] - 0.5) * 2,
                    "overall_sentiment": sentiment['label'],
                    "keywords": keywords,
                    "analyzed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                results.append(item)
            except Exception as e:
                print(f"âŒ í•­ëª© ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
                continue
        
        if not results:
            print(f"âš ï¸ ì²˜ë¦¬ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤: {category_path}")
            return
            
        # ê²°ê³¼ ì €ì¥
        try:
            with open(os.path.join(category_path, "blog_data_analyzed.json"), 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì¤‘ ì—ëŸ¬: {e}")
            return
        
        # í†µê³„ ì •ë³´ ìƒì„±
        self.generate_statistics(results, category_path)
        
    def generate_statistics(self, data, category_path):
        if not data:
            print(f"âš ï¸ í†µê³„ ìƒì„±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {category_path}")
            return
            
        try:
            # ê°ì„± ë¶„ì„ í†µê³„ ìƒì„±
            sentiment_counts = Counter(item['sentiment_analysis']['overall_sentiment'] for item in data)
            total = len(data)
            
            if total == 0:
                print(f"âš ï¸ ë°ì´í„°ê°€ ì—†ì–´ í†µê³„ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {category_path}")
                return
            
            stats = {
                "total_posts": total,
                "sentiment_distribution": {
                    sentiment: {
                        "count": count,
                        "percentage": (count / total) * 100
                    }
                    for sentiment, count in sentiment_counts.items()
                },
                "average_scores": {
                    "positive": sum(item['sentiment_analysis']['positive_score'] for item in data) / total,
                    "negative": sum(item['sentiment_analysis']['negative_score'] for item in data) / total,
                    "neutral": sum(item['sentiment_analysis']['neutral_score'] for item in data) / total
                }
            }
            
            # í†µê³„ ì •ë³´ ì €ì¥
            with open(os.path.join(category_path, "sentiment_stats.json"), 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ í†µê³„ ìƒì„± ì¤‘ ì—ëŸ¬: {e}")

if __name__ == "__main__":
    analyzer = SentimentAnalyzer()
    base_path = "result"
    
    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê°ì„± ë¶„ì„ ìˆ˜í–‰
    for category in os.listdir(base_path):
        category_path = os.path.join(base_path, category)
        if os.path.isdir(category_path):
            analyzer.process_category(category_path)
    
    print("\nâœ… ëª¨ë“  ê°ì„± ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!") 